{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "import wandb\n",
    "import tiktoken\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspaces/Wikistim-Summarization/api_key.txt', 'r') as file:\n",
    "    API_KEY = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/workspaces/Wikistim-Summarization/00_source_data/uncleaned_text.csv')\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "data['tokenized_text'] = data['full_article_text'].apply(lambda x: encoding.encode(x))\n",
    "data['count_of_tokens'] = data['tokenized_text'].apply(lambda x: len(x))\n",
    "longest_9 = data.sort_values(by='count_of_tokens', ascending=False).head(9)\n",
    "shortest_9 = data.sort_values(by='count_of_tokens', ascending=True).head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we are testing the difference between the GPT 4 Turbo and GPT 4 Regular models. The goal of this analysis is to understand if GPT 4 Turbo has greater accuracy than GPT 4 Regular to warrant the additional cost. Because GPT 4 Regular has a significantly shorter context window, the papers will be chunked For GPT-4 Regular. The approach involves posing specific questions to GPT-4 for each chunk and utilizing the generated answers as contextual input for subsequent chunks. This sequential questioning and context transfer method are employed to simulate a continuous understanding of the entire document. Each chunk's context is preserved by incorporating the model's responses from the previous iterations, allowing for a more cohesive analysis despite the model's limitations.\n",
    "\n",
    "This process ensures that, as we move through the document in chunks, GPT-4 Regular is provided with the necessary context from its preceding responses, aiming to maintain a coherent understanding of the entire paper. The goal is to assess how well GPT-4 Regular adapts to sequential questioning and context transfer, and whether it can effectively process information in this manner.\n",
    "\n",
    "Simultaneously, GPT-4 Turbo undergoes the same sequential questioning process, and its responses are compared with those of GPT-4 Regular. By evaluating the accuracy, coherence, and contextual relevance of both models in this sequential context transfer setting, we aim to understand if the enhanced capabilities of GPT-4 Turbo justify its potential additional cost in scenarios where sequential processing is crucial for maintaining context and generating accurate responses.\n",
    "\n",
    "\n",
    "\n",
    "We are testing the generation of two features: Number of Participants and Study Design. Specifically, we have two questions that we are posing to GPT 4 Turbo and GPT 4 Regular: \n",
    "* First read the following paper. Then guess at what the study design is and return that value. \n",
    "\n",
    "* First read the following paper. Then guess at how many participants were in the study and return that value. \n",
    "\n",
    "\n",
    "To evaluate performance of GPT 4 Turbo, we simply calculate the accuracy. For GPT 4 Regular, we calculate accuray using two different methodologies. One is binary and counts whether the model generates the correct answer from ANY chunk. The other counts the number of correctly generated answers across all of the chunks. If the model generates the correct answer more often then not, then that is considered a true positive. \n",
    "\n",
    "Below are the results followed by conclusions, next steps, and code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 4 Turbo\n",
    "\n",
    "| Data      | Study Design Accuracy | Number of Participants Accuracy |\n",
    "|-----------|------------------------|---------------------------------|\n",
    "| 9 Long    |          88.89%              |                 100%                |\n",
    "| 9 Short   |              88.89%          |                     77.76%            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 4 Regular\n",
    "\n",
    "| Data      | Study Design Accuracy (Correct Answer Anywhere) | Study Design Accuracy (By Vote) | Number of Participants Accuracy (Correct Answer Anywhere) | Number of Participants Accuracy (By Vote) |\n",
    "|-----------|--------------------------------------------------|----------------------------------|-----------------------------------------------------------|-------------------------------------------|\n",
    "| 9 Short    |   88.89%                                               |          77.78%                        |                     88.89%                                      |                    66.67%                       |\n",
    "| 9 Long   |          88.89%                                        |             77.78%                     |                                   100%                        |                             22.22%              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def chunk_text(text, max_tokens=4096):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in text.split(\". \"):  # Assuming sentences end with a period\n",
    "        if len(current_chunk) + len(sentence) < max_tokens:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def generate_prompt(chunk, context):\n",
    "    prompt = f'First read the following paper. Then guess at how many participants were in the study and return that value.\"{context}\" \"{chunk}\"'\n",
    "    return prompt\n",
    "\n",
    "def evaluate_model_on_dataset(api_key, model_name, prompt, dataset, column):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    evaluation_df = pd.DataFrame(columns=['correct_answer', 'final_gpt_answer', 'cosine_similarity'])\n",
    "\n",
    "    previous_answer = \"\"  # Initialize the previous answer\n",
    "\n",
    "    responses = []  # List to store responses for each chunk in each iteration\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        print('this is index: ', index)\n",
    "        full_text = row['full_article_text']\n",
    "\n",
    "        # Chunk the text into smaller pieces\n",
    "        text_chunks = chunk_text(full_text)\n",
    "\n",
    "        chunk_responses = []  # List to store responses for each chunk in this iteration\n",
    "\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            print(f'Processing chunk {i+1}/{len(text_chunks)}')\n",
    "            context = previous_answer  # Use the previous answer as context\n",
    "\n",
    "            try:\n",
    "                # Introduce a lag (e.g., 2 seconds) before making the API call\n",
    "                time.sleep(2)\n",
    "\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": generate_prompt(chunk, context)}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                correct_answer_value = dataset.loc[index, column]\n",
    "\n",
    "                # Extract the content from the response\n",
    "                gpt_answer_value =  completion.choices[0][\"message\"][\"content\"]\n",
    "                \n",
    "                cosine_similarity_value = 0\n",
    "\n",
    "                print(gpt_answer_value)\n",
    "\n",
    "                # Update the previous answer for the next iteration\n",
    "                previous_answer = gpt_answer_value\n",
    "\n",
    "                # Store the response for this chunk\n",
    "                chunk_responses.append(gpt_answer_value)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {i+1}: {str(e)}\")\n",
    "                # Handle the error as needed, e.g., log it or skip the chunk\n",
    "\n",
    "        # Store all chunk responses for this iteration\n",
    "        responses.append(chunk_responses)\n",
    "\n",
    "    # Update the final_gpt_answer in the DataFrame with the last response in each iteration\n",
    "    for i, iteration_responses in enumerate(responses):\n",
    "        evaluation_df.loc[evaluation_df.index.isin(range(i * len(text_chunks), (i + 1) * len(text_chunks))),\n",
    "                        'final_gpt_answer'] = iteration_responses[-1]\n",
    "\n",
    "    # Add the correct_answer values to the DataFrame\n",
    "    evaluation_df['correct_answer'] = dataset[column]\n",
    "\n",
    "    return evaluation_df, responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shortest_9 = shortest_9.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Clinical/Scientific Notes W. Thevathasan, FRACP* P. Mazzone, MD* A. Jha, MRCP A. Djamshidian, MD M. Dileone, MD V. Di Lazzaro, MD P. Brown, FRCP SPINAL CORD STIMULATION FAILED TO RELIEVE AKINESIA OR RESTORE LOCOMOTION IN PARKINSON DISEASE Dorsal column spinal stimulation in dopamine- depleted rodents was recently reported to disrupt pathologic corticostriatal synchronization, alleviate akinesia, and restore locomotion.1 This claim has prompted consideration that spinal stimulation “might become an efficient and less invasive alterna- tive for treatment of Parkinson disease (PD) in the future.” In this study, we investigated whether dorsal col- umn stimulation was of therapeutic benefit in 2 pa- tients with PD. Level of evidence. This study provides Class II evi- dence that for patients with moderate to severe mo- tor impairment from PD, high-frequency epidural cervical spinal cord stimulation does not significantly improve motor function as measured by the motor subsection of the Unified Parkinson’s Disease Rating Scale (UPDRS). Methods. Two patients with PD had spinal stimula- tors (Medtronic models 3487a or 3898) implanted surgically into the high cervical epidural space with- out complication, as described previously.2 Patient 1, 75 years old, had moderate motor impairment (off/on medication motor UPDRS � 30/18). Pa- tient 2, 77 years old, had more severe motor im- pairment (off/on medication UPDRS � 51/38) and was unable to walk without dopaminergic medication. Both patients met UK Brain Bank cri- teria for PD.3 Ten days postoperatively, both patients partici- pated in a double-blind crossover study of the motor effects of spinal stimulation. In an initial exploratory phase, an antiparkinsonian effect of spinal stimula- tion was sought over a range of frequencies (30–300 Hz) and intensities (up to 4.0 V and 240 �s). Having failed to establish a clear benefit for any particular set of parameters, for the purposes of the study, patient 1 was assigned to receive 130 Hz stimulation (a com- mon frequency employed for deep brain stimulation and beneficial in the rodent study) and patient 2 300 Hz (the most effective frequency in the rodent study). At those frequencies, stimulation intensity thresholds for the production of paresthesiae were established. Patients were then assessed in the off medication state (after overnight withdrawal of do- paminergic medication) during 3 conditions: off stimulation, subthreshold stimulation (without paresthesiae), and suprathreshold stimulation (with paresthesiae). Subthreshold and suprath- reshold stimulation parameters were as follows: pa- tient 1 130 Hz/2 V/240 �s and 130 Hz/3 V/240 �s; patient 2 300 Hz/3 V/200 �s and 300 Hz/4 V/200 �s. The ordering of conditions was counterbalanced so that each patient had 9 assessments. Outcomes were assessed �20 minutes after switching stimula- tion conditions (a period sufficient for most parkin- sonian signs to return following subthalamic stimulation).4 The primary outcome measure was the motor subsection of the UPDRS (motor UPDRS, mean score of 2 blinded neurologists rating from vid- eotape).5 Secondary outcomes included the timed Hand-Arm Movement test (number of alternating movements between 2 fixed points 30 cm apart in 30 s, mean of both sides), timed foot tapping score (number of foot taps in 30 s, mean of both sides), and time to walk 10 m test.6 Paresthesiae were rated by the subjects with a Visual Analogue Scale (10 cm VAS).7 Standard protocol approvals, registrations, and patient consents. Ethical approval was granted by the local ethics committee and patients gave written informed consent. Results. Startle was not observed with changes to stimulation in either patient. VAS scores for the su- prathreshold condition were greater than the off stimulation condition (mean 2.1 vs 0.68 cm, Z � �2.03, p � 0.04 Wilcoxon). VAS scores for the sub- threshold and off stimulation conditions were not different (1.9 vs 0.68 cm, Z � �0.94, p � 0.35 Wilcoxon).',\n",
       " 'There was strong interrater reliability be- tween the motor UPDRS scoring of the 2 blinded neurologists (Spearman rho � 0.92). There was no difference in the primary outcome measure of motor UPDRS between stimulation con- ditions (�2 � 1.65, p � 0.44 Friedman). There were Neurology 74 April 20, 2010 1325 no differences in any other outcome measure with stimulation (table). Dorsal column stimulation did not restore the locomotion of patient 2, who re- mained unable to walk in the off medication state. Discussion. We assessed whether the benefits of spinal stimulation, recently reported in parkinso- nian rodents, could be translated to patients with PD. While it is not possible to prove that spinal stimulation in some form will never be of thera- peutic value, we found that in 2 subjects, dorsal column stimulation delivered continuously at clinically acceptable stimulation intensities was not clinically beneficial. The difference in outcomes between the 2 studies may reflect differences between species, models, or, alternatively, the differing methods of stimulation. In the rodent study, suprathreshold stimulation at intensities sufficient to produce startle was applied intermittently in 30–60 s bursts.1 Only during and shortly after stimulation did beneficial effects occur, which included reduced beta oscillations, alleviation of akinesia, and increased locomotion. The authors postulated that dorsal column stimulation may di- rectly desynchronize pathologic oscillations, thereby providing a state permissive for movement. However, an alternative mechanism is simply that bursts of startling stimulation increased arousal, thereby precipitating movement, as occurs with par- adoxical kinesis. The authors of the rodent study controlled for startle-related phenomena by applying startling trigeminal stimulation, but at lower inten- sity thresholds than applied for spinal stimulation. Locomotion not only increased in parkinsonian ro- dents, but in healthy control rodents as well. Cru- cially, the effect of chronic stimulation was not reported so it is possible that habituation may have occurred. Intermittent startling spinal stimulation that briefly precipitates locomotion by increasing arousal would not be an acceptable therapy in patients. However, we found that spinal stimulation admi- nistered continuously, at the same frequencies and at clinically acceptable intensities, failed to improve motor deficits in 2 carefully evaluated patients with PD. *These authors contributed equally. From the Sobell Department of Motor Neuroscience and Movement Disorders (W.T., A.J., A.D., P.B.), Institute of Neurology, London, UK; Department of Neurosurgery (P.M.), Ospedale CTO Andrea Alesini, Rome; and Institute of Neurology (M.D., V.D.), Universita` Cattolica, Rome, Italy. Study funding: Supported by the Medical Research Council (UK). Disclosure: Dr. Thevathasan has received funding for travel from Medtronic, Inc. Dr. Mazzone has received funding for travel from Medtronic, Inc., and serves on the editorial board of Neuromodu- lation. Dr. Jha has received fellowship support from the Parkinson’s Disease Society (UK). Dr. Djamshidian and Dr. Dileone report no disclosures. Dr. Di Lazzaro serves on scientific advisory boards for Medtronic, Inc.; serves on the editorial boards of Clinical Neuro- physiology and Brain Stimulation and as editor of Case Report in Medicine; and receives research support from Institut des Recherches Internationales Servier. Dr. Brown has served as a consultant to Medtronic, Inc. Received September 4, 2009. Accepted in final form January 28, 2010. Address correspondence and reprint requests to Prof. Peter Brown, Sobell Department of Motor Neuroscience and Movement Disor- ders, UCL Institute of Neurology, 33 Queen Square, London, WCIN 3BG, UK; pbrown@ion.ucl.ac.uk Copyright © 2010 by AAN Enterprises, Inc. AUTHOR CONTRIBUTIONS Statistical analysis was conducted by Dr. W. Thevathasan and Dr. P. Brown. 1. Fuentes R, Petersson P, Siesser WB, Caron MG, Nicolelis MA.',\n",
       " 'Spinal cord stimulation restores locomotion in ani- mal models of Parkinson’s disease. Science 2009;323: 1578–1582. 2. Insola A, Padua L, Mazzone P, Valeriani M. Unmasking of presynaptic and postsynaptic high-frequency oscillations in epidural cervical somatosensory evoked potentials dur- ing voluntary movement. Clin Neurophysiol 2008;119: 237–245. 3. Hughes AJ, Daniel SE, Kilford L, Lees AJ. Accuracy of clinical diagnosis of idiopathic Parkinson’s disease: a clinico-pathological study of 100 cases. J Neurol Neuro- surg Psychiatry 1992;55:181–184. 4. Temperli P, Ghika J, Villemure JG, Burkhard PR, Bogousslavsky J, Vingerhoets FJ. How do parkinsonian Table Outcomes from dorsal column stimulation in 2 patients with Parkinson diseasea Motor UPDRS (score/104) Timed 10-meter walk (s) Timed hand–arm movements (n/30 s) Timed lower limb tapping (n/30 s) Baseline (off stimulation) 37.8 (11.5) 5.5 (1.2) 30.3 (15.9) 54.2 (21.9) Subthreshold stimulation 35.4 (12.5) 5.4 (0.4) 32.7 (18.0) 54.2 (22.8) Suprathreshold stimulation 37.3 (10.5) 5.6 (1.0) 31.2 (16.3) 52.0 (24.7) Friedman (p value) 0.44 0.72 0.32 0.85 Abbreviation: UPDRS � Unified Parkinson’s Disease Rating Scale. aValues are means (SD). There were 3 assessments per patient per condition. The timed 10-m walk results represent the performance of patient 1 only (patient 2 was persistently unable to perform the task). 1326 Neurology 74 April 20, 2010 signs return after discontinuation of subthalamic DBS? Neurology 2003;60:78–81. 5. Fahn S, Elton RL, UPDRS program members. Unified Parkinson’s Disease Rating Scale. In: Fahn S, Marsden CD, Calne DB, Goldstein M, ed. Recent Developments in Parkinson’s Disease. Florham Park, NJ: Macmillan Healthcare Information; 1987:153–163, 293–304. 6. Defer GL, Widner H, Marie RM, Remy P, Levivier M. Core assessment program for surgical interventional thera- pies in Parkinson’s disease (CAPSIT-PD). Mov Disord 1999;14:572–584. 7. Huskisson E. Visual analogue scales. In: Pain Measure- ment and Assessment. New York: Raven Press; 1983. Editor’s Note to Authors and Readers: Levels of Evidence coming to Neurology® Effective January 15, 2009, authors submitting Articles or Clinical/Scientific Notes to Neurology® that report on clinical therapeutic studies must state the study type, the primary research question(s), and the classification of level of evidence assigned to each question based on the classification scheme requirements shown below (left). While the authors will initially assign a level of evidence, the final level will be adjudicated by an independent team prior to publication. Ultimately, these levels can be translated into classes of recommendations for clinical care, as shown below (right). For more information, please access the articles and the editorial on the use of classification of levels of evidence published in Neurology.1-3 REFERENCES 1. French J, Gronseth G. Lost in a jungle of evidence: we need a compass. Neurology 2008;71:1634–1638. 2. Gronseth G, French J. Practice parameters and technology assessments: what they are, what they are not, and why you should care. Neurology 2008;71:1639–1643. 3. Gross RA, Johnston KC. Levels of evidence: taking Neurology® to the next level. Neurology 2008;72:8–10. Neurology 74 April 20, 2010 1327 .']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_text(test_shortest_9['full_article_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is index:  14\n",
      "Processing chunk 1/3\n",
      "Error processing chunk 1: 'Choice' object is not subscriptable\n",
      "Processing chunk 2/3\n",
      "Error processing chunk 2: 'Choice' object is not subscriptable\n",
      "Processing chunk 3/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dataset \u001b[39m=\u001b[39m test_shortest_9  \n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m column \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNumber in study\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m result_df, resukt_responses \u001b[39m=\u001b[39m evaluate_model_on_dataset(API_KEY, model_name, prompt, dataset, column)\n",
      "\u001b[1;32m/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# Introduce a lag (e.g., 2 seconds) before making the API call\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m2\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     completion \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: generate_prompt(chunk, context)}\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m         ]\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     correct_answer_value \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mloc[index, column]\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m# Extract the content from the response\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/resources/chat/completions.py:556\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    514\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    555\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    557\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    558\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    559\u001b[0m             {\n\u001b[1;32m    560\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    561\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    562\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    563\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    564\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    565\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    566\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    567\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    568\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    569\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    570\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    571\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    572\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    573\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    574\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    575\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    576\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    577\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    578\u001b[0m             },\n\u001b[1;32m    579\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    580\u001b[0m         ),\n\u001b[1;32m    581\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    582\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    583\u001b[0m         ),\n\u001b[1;32m    584\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    585\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    586\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    587\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    840\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_base_client.py:858\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    857\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 858\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(request, auth\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_auth, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m    859\u001b[0m     log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    860\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mHTTP Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, request\u001b[39m.\u001b[39mmethod, request\u001b[39m.\u001b[39murl, response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mreason_phrase\n\u001b[1;32m    861\u001b[0m     )\n\u001b[1;32m    862\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    902\u001b[0m     request,\n\u001b[1;32m    903\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    904\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    905\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    906\u001b[0m )\n\u001b[1;32m    907\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    930\u001b[0m         request,\n\u001b[1;32m    931\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    932\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    934\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1004\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    230\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    213\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/ssl.py:1259\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1256\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1257\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1258\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1259\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1260\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1133\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1134\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example usage \n",
    "model_name = \"gpt-3.5-turbo\" \n",
    "prompt = \"First read the following paper. Then guess at how many participants were in the study and return that value.\"\n",
    "dataset = test_shortest_9  \n",
    "column = 'Number in study' \n",
    "\n",
    "result_df, resukt_responses = evaluate_model_on_dataset(API_KEY, model_name, prompt, dataset, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.base.Index"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_9.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([14, 16, 4, 12, 2, 13, 17, 10, 11], dtype='int64')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_9.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       " ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       " ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       " ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       " ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resukt_responses[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_9.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.index(value, start=0, stop=9223372036854775807, /)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resukt_responses.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_find = 'ChatCompletionMessage(content=\\'It is not possible to determine the exact number of participants in the study based on the provided information.\\', role=\\'assistant\\', function_call=None, tool_calls=None)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       " ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       " ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       " ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       " ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resukt_responses[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None)\" is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bprobable-capybara-x7xw5vv54pxc975j/workspaces/Wikistim-Summarization/10_code/gpt_modeling_clean.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m resukt_responses[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mindex(to_find)\n",
      "\u001b[0;31mValueError\u001b[0m: \"ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None)\" is not in list"
     ]
    }
   ],
   "source": [
    "resukt_responses[1].index(to_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.index(value, start=0, stop=9223372036854775807, /)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(resukt_responses).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ChatCompletionMessage(content='Based on the information provided, there were 2 participants in the study.', role='assistant', function_call=None, tool_calls=None),\n",
       "  ChatCompletionMessage(content='Based on the information provided, it is not possible to determine the exact number of participants in the study.', role='assistant', function_call=None, tool_calls=None),\n",
       "  ChatCompletionMessage(content='Based on the information provided, it is not possible to determine the exact number of participants in the study.', role='assistant', function_call=None, tool_calls=None)],\n",
       " [ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       "  ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       "  ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       "  ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None),\n",
       "  ChatCompletionMessage(content='It is not possible to determine the exact number of participants in the study based on the provided information.', role='assistant', function_call=None, tool_calls=None)]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resukt_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "for respon in resukt_responses:\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>final_gpt_answer</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   correct_answer final_gpt_answer cosine_similarity\n",
       "14              2              NaN               NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
